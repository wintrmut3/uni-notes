\documentclass[10pt, twocolumn]{report}
\usepackage{amssymb}
\usepackage{braket}

\usepackage{amsmath}

\begin{document}
  \chapter{Vector Spaces}
  \section {The Fundamental theorem of Subspaces}
  Let U be a subspace in $\mathbb{R}^n$ with a spanning set of {$\vec x_1 ... \vec x_m$}. If {$\vec{y_1} ... \vec y_k$} is linearly independent, $k\leq m$\\\\
  The proof of this is built on the replacement lemma: If we are given a set of vectors $\vec U_1 \to \vec U_l$ and $v = a_1 \vec U_1 ... a_l \vec U_l$ with $a_1 \neq 0$, then the span of $u_1 \to u_l$ is equal to the span of {$V, U_2 ... U_l$}. We can collect terms and factor out to prove this. \\\\
  Definition of Equality of U and W: For every $\vec x$, $\vec x$ is in subspace U iff $\vec x$ is in W, ie. $U \subset W$. Also, for x to be in a set, it must fulfil the properties of the set.

  \subsection{Bases and Dimension}
  A \textbf{Basis} is a set that first spans a given subspace and second is linearly independent. \\ The \textbf{Invariance Thm} suggests that if there are two bases of U, then they have the same number of vectors.\\\\
  The \textbf{Dimension} of any subspace except $\vec 0$ is the number of vectors in any of its bases.


  \section{Orthogonalization}
  (This is continued as of November 14th's lecture)
Matrix Template
 $$\begin{bmatrix}
  a & b & c \\
  d & e & f \\
  g & h & i
\end{bmatrix}$$

\textbf{Normalizing} an orthogonal set ${x_1 ... x_k}$ means to use ${\frac{\vec{x_1}}{|\vec{x_1}|} ... \frac{x_k}{|x_k|}}$ \\\\

\textbf{Pythagoreas in $\mathbb{R}^n$} if {$\vec{x_1} ... \vec{x_n}$} is orthogonal, then $|\vec{x_1} + \vec{x_k}|^2$ = $\vec{|x_1|}^2 ... + \vec{|x_k|}^2$ \\(Prove this for homework, as a dot product against itself.)

\subsection{The Expansion Theorem}
If $x_1$ to $x_k$ is an orthogonal basis of a subspace $U$ then for any $\vec{x}$ in the subspace, we can write $\vec{x}$ is equal to the
$$ \vec{x} = \sum_{i = 1}^{k} \frac{\vec{f_i}\cdot \vec{x}}{\vec{|f_i|}^2}$$ is $$\sum_{i=1}^{k} proj_{f_i} \vec{x}$$ \\

Proof: $\vec{x} = \sum_i a_i\vec{f_i}$ \\ For any $j$, $\vec{x}\cdot\vec{f_j}$ = $\sum_i a_i\vec{f_i} \cdot \vec{f_j}$, which will give $a_j(\vec{f_j}\cdot\vec{f_j})$ which is the magnitude squared. We can use this for all j. \\

$\bigg\{\begin{bmatrix} 1 \\ 1 \\ -1 \end{bmatrix}, \begin{bmatrix} 2 \\ -1 \\ 1 \end{bmatrix}, \begin{bmatrix} 0 \\ -1 \\ -1 \end{bmatrix}\bigg\}$ is orthogonal.

Write $\begin{bmatrix} 5 \\ -1 \\ 3 \end{bmatrix}$ as a linear combination.

Prior to this, we had to assign parameters and solve, but now we can project $\begin{bmatrix} 5 \\ -1 \\ 3 \end{bmatrix}$ onto each of our three orthogonal vectors, and sum it. The fractional term from the projection $\frac{\begin{bmatrix} 5 \\ -1 \\ 3 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \\ -1 \end{bmatrix}}{\bigg|\begin{bmatrix} 1 \\ 1 \\ -1 \end{bmatrix}\bigg|^2} \begin{bmatrix} 1 \\ 1 \\ -1 \end{bmatrix} + ...$

plus the rest of the ortogonal set.
$$= \frac{1}{3} \begin{bmatrix} 1 \\ 1 \\ -1 \end{bmatrix} + ...$$

\section{5.4 - newish Subspaces}
Given an $m\times n$ matrix A
\begin{itemize}
  \item row(A) = span of the rows of A. This is its own object.
  \item col(A) = span of the cols of A. = the set of everything of the form where c is col {$a_1 \vec{c_1} + ... + a_n \vec{c_n}$| $a_1 \to a_n$ are real} \\ $=\{A\vec{x}| \vec{x} \in \mathbb{R}^n\}$ which is actually the image of A!
\end{itemize}

If A is equiv to B by any number of elementary row operations, their row spaces are also equivalent. (row(A) = row(B)). The proof is done in three parts, and we need to prove that none of the three row ops change anything. \\\\ Let A = $\begin{bmatrix} r_1 \\ r_2 \\ ... \\ r_n \end{bmatrix}, row(A) = span(\vec{r_1} + \vec{r_2} + ... \vec{r_m})$
\begin{itemize}
  \item Rearranging them just changes the order that you get somewhere
  \item Multiplying by an arbitrary scalar can be swallowed by the coefficient from span.
  \item Addition of a scalar multiple of anything can be factored out and summed somewhere else.
  \item The proof of this is left as an exercise to the reader.
\end{itemize}
$\therefore |A\text{\char `\~} R$ with R in row echelon form, then row(A) = row (R). \\\\ From yesterday, the nonzero rows of R are independent, therefore they form a basis for the rowspace of R, therefore they form a basis for the rowspace of A, so their number is constant. \\\\ Therefore, the number of nonzero rows in R, which is equal to the number of nonleading variables is a constant. This is called the \textbf{rank} of the matrix. So, the rank exists!\\

So the number of parameters in $A\vec{x} = \vec{0}$ is constant, so $n-rank(A)$ = $dim(null(A))$.

\subsection {Useful Algorithms}
Given vectors \{$\vec x_1, \vec x_2 ... \vec x_n$\}, find a basis for the vector space they span. \\\\ \textbf{Method I}\\
write each vector $\vec x_i$ as a row in a, and reduce it to REF. Then, take the nonzero rows as your basis. This yields a basis easily, but the basis will not be a subset of your initial vectors. However, you may take the rows in A corresponding to the rows in R that are nonzero, given that you have made no swaps or reversed them in A while doing them in R. \\\\
 \textbf{Method II}\\
 Write the vectors as columns of A, and reduce to REF. The basis is the collection of columns in A, that correspond to the columns with leading terms (ones) in R. \\\\

 To get a basis of the null space, write your vectors as columns in a matrix, and row reduce. Then, write your solutions as parameters. The basic solutions are the basis of the null space.

 \subsection{Dimension Theorem, Rank + Nullity}
 If A is mxn with rank r, the dimension of the null space of a plus the dimension of the image space of A is equal to n, the number of columns. This is because the image space corresponds to the leading variables and null to the nonleading variables, and each column must be one or another. \\\\ This lets us know the possible dimensions of both spaces, given just the size of the matrix. For example, for a 3x5 A, \\dim(null(A))+dim(im(A))=5. However, since im(A) is in $\mathbb{R}^3$, $dim(im(A)) \leq 3$. Recall again that the dimension of the image is the dimension of the col space, which has 3 entries per column, so it is in R3. So, then $dim(null(A))\geq 2$

\subsection{Extending into a Basis}
To extend a set of vectors into a basis for some $R^n$, we add e1 e2 ... en to the matrix, augmenting it. Then, we row reduce it and take the columns of augmented A corresponding to the columns in $A_{REF}$ with leading ones.
\subsection{More Properties}
- For invertible U and V, $rank (AV) = rank (UA)$.\\
- The basis of im(A) is the set of columns corresponding to the leading ones in a row echelon equivalent of A.\\
- A basis of null(A) is the (n-r) basic solutions to $A\vec x = \vec 0$. Solve the homogenous system and take the basic solutions for the parameters!

\section{Equivalent Statements for a M x N matrix A}
Theorem 1\\
The following are equivalent:
\begin{enumerate}
    \item rank(A) = n
    \item The rows of A span $R^n$
    \item The columns of A in $R_m$ are linearly independent
    \item $A^T A$ is invertible (Symmetric matrix)
    \item There exists some C such that CA = $I_n$
    \item $A\vec x = \vec 0$ has only the trivial solution.
\end{enumerate}
Theorem 2\\
\begin{enumerate}
    \item rank(A) = m
    \item The cols of A span $R^m$
    \item The rows of A in $R_n$ are linearly independent
    \item $A^T A$ is invertible (Symmetric matrix)
    \item There exists some D such that AD = $I_m$
    \item $A\vec x = \vec b$ has a unique solution for every $\vec b \in \mathbb R^n$
\end{enumerate}
\section{Similarity}
$A \sim B$ (A is similar to B) if there exists some invertible P, such that $B = P^-1 A P$. Some properties:
\begin{itemize}
    \item $A \sim A$
    \item $A \sim B \Rightarrow B \sim A$
    \item $|A \sim B, B \sim C \Rightarrow C \sim A$
\end{itemize}
The Trace (tr(A)) of a matrix A gives the sum of the main diagonal entries $\sum a_{i,i}$. This may or may not be very useful. tr(AB) = tr(BA) \newpage
Theorem: if $A \sim B$ for square A, B, they share:
\begin{itemize}
    \item Trace
    \item Determinants
    \item Characteristic Polynomials
    \item Eigenvalues
\end{itemize}
but NOT EIGENVECTORS! \\\\
Theorem: A is diagonalizable if there exists a basis of $R^n$ composed of eigenvectors. Also, if \{$\vec x_1 ... \vec x_w$\} for some DISTINCT eigenvalues $\lambda_1 ... \lambda_k$, the set \{$\vec x_1 ... \vec x_k$\} is linearly independent. If k = n, it is diagonalizable.  \\\\
Thm: A is diagonalizable if and olny if the number of basic eigenvalues $E_\lambda A$ is equal to the multiplicity of $\lambda$ in $C_A x$.
\section{Least Squares Approximations}
For $A\vec x = \vec b$, we can solve it if it's consistent. If it's inconsistent, we can use LSA to get the closest solutions for $\vec x$. \\\\ Geometrically, inconsistency appears as a vector outside of the plane formed by im(A). It starts on the plane somewhere and comes off it. The point made by dropping the perpendicular from b (vector off the plane) onto the plane of im(A), which we'll call z, is the closest we can get to a solution. \\\\This gives us the normal equations $$A^T \vec b = (A^TA)\vec z$$. We know that a symmetric matrix is invertible, and we are solving for $\vec z$. We can use $(A^TA)^{-1}A^T \vec b = \vec z$ to solve directly, minimizing $|\vec b - A\vec z|$. This also tells use we have a unique answer, as we have a unique inverse matrix for the symmetric matrix.

\end{document}
