\documentclass[10pt, twocolumn]{report}
\usepackage{amssymb}
\usepackage{braket}

\usepackage{amsmath}

\begin{document}
  \chapter{Vector Spaces}
  \section {The Fundamental theorem of Subspaces}
  Let U be a subspace in $\mathbb{R}^n$ with a spanning set of {$\vec x_1 ... \vec x_m$}. If {$\vec{y_1} ... \vec y_k$} is linearly independent, $k\leq m$\\\\
  The proof of this is built on the replacement lemma: If we are given a set of vectors $\vec U_1 \to \vec U_l$ and $v = a_1 \vec U_1 ... a_l \vec U_l$ with $a_1 \neq 0$, then the span of $u_1 \to u_l$ is equal to the span of {$V, U_2 ... U_l$}. We can collect terms and factor out to prove this. \\\\
  Definition of Equality of U and W: For every $\vec x$, $\vec x$ is in subspace U iff $\vec x$ is in W, ie. $U \subset W$. Also, for x to be in a set, it must fulfil the properties of the set.

  \subsection{Bases and Dimension}
  A \textbf{Basis} is a set that first spans a given subspace and second is linearly independent. \\ The \textbf{Invariance Thm} suggests that if there are two bases of U, then they have the same number of vectors.\\\\
  The \textbf{Dimension} of any subspace except $\vec 0$ is the number of vectors in any of its bases.


  \section{Orthogonalization}
  (This is continued as of November 14th's lecture)
Matrix Template
 $$\begin{bmatrix}
  a & b & c \\
  d & e & f \\
  g & h & i
\end{bmatrix}$$

\textbf{Normalizing} an orthogonal set ${x_1 ... x_k}$ means to use ${\frac{\vec{x_1}}{|\vec{x_1}|} ... \frac{x_k}{|x_k|}}$ \\\\

\textbf{Pythagoreas in $\mathbb{R}^n$} if {$\vec{x_1} ... \vec{x_n}$} is orthogonal, then $|\vec{x_1} + \vec{x_k}|^2$ = $\vec{|x_1|}^2 ... + \vec{|x_k|}^2$ \\(Prove this for homework, as a dot product against itself.)

\subsection{The Expansion Theorem}
If $x_1$ to $x_k$ is an orthogonal basis of a subspace $U$ then for any $\vec{x}$ in the subspace, we can write $\vec{x}$ is equal to the
$$ \vec{x} = \sum_{i = 1}^{k} \frac{\vec{f_i}\cdot \vec{x}}{\vec{|f_i|}^2}$$ is $$\sum_{i=1}^{k} proj_{f_i} \vec{x}$$ \\

Proof: $\vec{x} = \sum_i a_i\vec{f_i}$ \\ For any $j$, $\vec{x}\cdot\vec{f_j}$ = $\sum_i a_i\vec{f_i} \cdot \vec{f_j}$, which will give $a_j(\vec{f_j}\cdot\vec{f_j})$ which is the magnitude squared. We can use this for all j. \\

$\bigg\{\begin{bmatrix} 1 \\ 1 \\ -1 \end{bmatrix}, \begin{bmatrix} 2 \\ -1 \\ 1 \end{bmatrix}, \begin{bmatrix} 0 \\ -1 \\ -1 \end{bmatrix}\bigg\}$ is orthogonal.

Write $\begin{bmatrix} 5 \\ -1 \\ 3 \end{bmatrix}$ as a linear combination.

Prior to this, we had to assign parameters and solve, but now we can project $\begin{bmatrix} 5 \\ -1 \\ 3 \end{bmatrix}$ onto each of our three orthogonal vectors, and sum it. The fractional term from the projection $\frac{\begin{bmatrix} 5 \\ -1 \\ 3 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \\ -1 \end{bmatrix}}{\bigg|\begin{bmatrix} 1 \\ 1 \\ -1 \end{bmatrix}\bigg|^2} \begin{bmatrix} 1 \\ 1 \\ -1 \end{bmatrix} + ...$

plus the rest of the ortogonal set.
$$= \frac{1}{3} \begin{bmatrix} 1 \\ 1 \\ -1 \end{bmatrix} + ...$$

\section{5.4 - newish Subspaces}
Given an $m\times n$ matrix A
\begin{itemize}
  \item row(A) = span of the rows of A. This is its own object.
  \item col(A) = span of the cols of A. = the set of everything of the form where c is col {$a_1 \vec{c_1} + ... + a_n \vec{c_n}$| $a_1 \to a_n$ are real} \\ $=\{A\vec{x}| \vec{x} \in \mathbb{R}^n\}$ which is actually the image of A!
\end{itemize}

If A is equiv to B by any number of elementary row operations, their row spaces are also equivalent. (row(A) = row(B)). The proof is done in three parts, and we need to prove that none of the three row ops change anything. \\\\ Let A = $\begin{bmatrix} r_1 \\ r_2 \\ ... \\ r_n \end{bmatrix}, row(A) = span(\vec{r_1} + \vec{r_2} + ... \vec{r_m})$
\begin{itemize}
  \item Rearranging them just changes the order that you get somewhere
  \item Multiplying by an arbitrary scalar can be swallowed by the coefficient from span.
  \item Addition of a scalar multiple of anything can be factored out and summed somewhere else.
  \item The proof of this is left as an exercise to the reader.
\end{itemize}
$\therefore |A\text{\char `\~} R$ with R in row echelon form, then row(A) = row (R). \\\\ From yesterday, the nonzero rows of R are independent, therefore they form a basis for the rowspace of R, therefore they form a basis for the rowspace of A, so their number is constant. \\\\ Therefore, the number of nonzero rows in R, which is equal to the number of nonleading variables is a constant. This is called the \textbf{rank} of the matrix. So, the rank exists!\\

So the number of parameters in $A\vec{x} = \vec{0}$ is constant, so $n-rank(A)$ = $dim(null(A))$.

\subsection {Useful Algorithms}
Given vectors \{$\vec x_1, \vec x_2 ... \vec x_n$\}, find a basis for the vector space they span. \\\\ \textbf{Method I}\\
write each vector $\vec x_i$ as a row in a, and reduce it to REF. Then, take the nonzero rows as your basis. This yields a basis easily, but the basis will not be a subset of your initial vectors. However, you may take the rows in A corresponding to the rows in R that are nonzero, given that you have made no swaps or reversed them in A while doing them in R. \\\\
 \textbf{Method II}\\
 Write the vectors as columns of A, and reduce to REF. The basis is the collection of columns in A, that correspond to the columns with leading terms (ones) in R. \\\\

 To get a basis of the null space, write your vectors as columns in a matrix, and row reduce. Then, write your solutions as parameters. The basic solutions are the basis of the null space.

 \subsection{Dimension Theorem, Rank + Nullity}
 If A is mxn with rank r, the dimension of the null space of a plus the dimension of the image space of A is equal to n, the number of columns. This is because the image space corresponds to the leading variables and null to the nonleading variables, and each column must be one or another. \\\\ This lets us know the possible dimensions of both spaces, given just the size of the matrix. For example, for a 3x5 A, \\dim(null(A))+dim(im(A))=5. However, since im(A) is in $\mathbb{R}^3$, $dim(im(A)) \leq 3$. Recall again that the dimension of the image is the dimension of the col space, which has 3 entries per column, so it is in R3. So, then $dim(null(A))\geq 2$

\subsection{Extending into a Basis}
To extend a set of vectors into a basis for some $R^n$, we add e1 e2 ... en to the matrix, augmenting it. Then, we row reduce it and take the columns of augmented A corresponding to the columns in $A_{REF}$ with leading ones.
\subsection{More Properties}
- For invertible U and V, $rank (AV) = rank (UA)$.\\
- The basis of im(A) is the set of columns corresponding to the leading ones in a row echelon equivalent of A.\\
- A basis of null(A) is the (n-r) basic solutions to $A\vec x = \vec 0$. Solve the homogenous system and take the basic solutions for the parameters!

\section{Equivalent Statements for a M x N matrix A}
Theorem 1\\
The following are equivalent:
\begin{enumerate}
    \item rank(A) = n
    \item The rows of A span $R^n$
    \item The columns of A in $R_m$ are linearly independent
    \item $A^T A$ is invertible (Symmetric matrix)
    \item There exists some C such that CA = $I_n$
    \item $A\vec x = \vec 0$ has only the trivial solution.
\end{enumerate}
Theorem 2\\
\begin{enumerate}
    \item rank(A) = m
    \item The cols of A span $R^m$
    \item The rows of A in $R_n$ are linearly independent
    \item $A^T A$ is invertible (Symmetric matrix)
    \item There exists some D such that AD = $I_m$
    \item $A\vec x = \vec b$ has a unique solution for every $\vec b \in \mathbb R^n$
\end{enumerate}
\section{Similarity}
$A \sim B$ (A is similar to B) if there exists some invertible P, such that $B = P^-1 A P$. Some properties:
\begin{itemize}
    \item $A \sim A$
    \item $A \sim B \Rightarrow B \sim A$
    \item $|A \sim B, B \sim C \Rightarrow C \sim A$
\end{itemize}
The Trace (tr(A)) of a matrix A gives the sum of the main diagonal entries $\sum a_{i,i}$. This may or may not be very useful. tr(AB) = tr(BA) \newpage
Theorem: if $A \sim B$ for square A, B, they share:
\begin{itemize}
    \item Trace
    \item Determinants
    \item Characteristic Polynomials
    \item Eigenvalues
\end{itemize}
but NOT EIGENVECTORS! \\\\
Theorem: A is diagonalizable if there exists a basis of $R^n$ composed of eigenvectors. Also, if \{$\vec x_1 ... \vec x_w$\} for some DISTINCT eigenvalues $\lambda_1 ... \lambda_k$, the set \{$\vec x_1 ... \vec x_k$\} is linearly independent. If k = n, it is diagonalizable.  \\\\
Thm: A is diagonalizable if and olny if the number of basic eigenvalues $E_\lambda A$ is equal to the multiplicity of $\lambda$ in $C_A x$.
\section{Least Squares Approximations}
For $A\vec x = \vec b$, we can solve it if it's consistent. If it's inconsistent, we can use LSA to get the closest solutions for $\vec x$. \\\\ Geometrically, inconsistency appears as a vector outside of the plane formed by im(A). It starts on the plane somewhere and comes off it. The point made by dropping the perpendicular from b (vector off the plane) onto the plane of im(A), which we'll call z, is the closest we can get to a solution. \\\\This gives us the normal equations $$A^T \vec b = (A^TA)\vec z$$. We know that a symmetric matrix is invertible, and we are solving for $\vec z$. We can use $(A^TA)^{-1}A^T \vec b = \vec z$ to solve directly, minimizing $|\vec b - A\vec z|$. This also tells use we have a unique answer, as we have a unique inverse matrix for the symmetric matrix.
\subsection{Abstract LSA}
Given a set of data points $\set{(x_1, y_1) ... (x_k, y_k)}$, say y is a linear combination of some functions $\set{f_1 ... f_l}$, for a linear combination there are only two functions $f_1 = 1, f_2 = x$. We take $x = \begin{bmatrix}(a_1 \\ a_2 \\...\\a_k)\end{bmatrix}$ $y= \begin{bmatrix}(y_1 \\ y_2 \\...\\y_k)\end{bmatrix}$. (We're using x as a coefficient vector).\\\\ We generate a matrix M, where $m_{ij}$ has $i = $entry of x, $j = $index of f, so that $m_{ij} = f_j(x_i)$
The columns are constant f, and the rows are constant x. $$ M = \begin{bmatrix}(f_1(x_1) & f_2(x_1) & ... & f_l(x_1)\\ f_1(x_2) & f_2(x_2) & ... & f_l(x_2)\\...\\ f_1(x_k) & f_2(x_k) & ... & f_l(x_k)\end{bmatrix} = [f_j(x_i)]$$ \\\\ Say we're taking a linear regression for some linear system, that we know is inconsistent (given the same input, we get multiple outputs...), we can generate some x = $[a $ $ b]^T$ y = $[y_1 y_2 y_3]^T$ $f_1 = 1$ $f_2 = x$, we end up with some M. \\\\ This can be constructed by taking the initial row as constant 1, and the second row as our inputs $x_i$, as $f_2$ is our second row and we're just copying our x. This is multiplied against $[a $ $b]^T$, so we end up with $\vec y = M\vec x$. \\\\
We know that through the normal equations we get our closest point, the projection, by $M^TM\vec x = M^T\vec y$, where we multiply both sides by $M^T$ on the left, and using this matrix we solve for a and b, which we put into $y = a + bx$. \\\\ A similar process may be carried out for $y = a + bx + cx^2$.
\section{Orthogonality Again}
If U is spanned by some $\vec f_1 ... \vec f_k$ with this spanning set orthogonal, $f_i \cdot f_j = 0 | i\neq j$. (And $f_i \neq \vec 0$), the zero vector is not in this orthogonal set. If $\vec x$ is in U, we can express it as $x - \sum_i proj_{f_i}\vec x. $ Intuitively, we're taking out (dimensional) components of x. Let's call this x - dimensions $f_{k+1}$. If it's 0, then x is already in the set (we can express it as a linear combination onto each of the dimensions. It'll make a set that's augmented and orthogonal still, if we add $f_{k+1}$ into our original set, as long as x isn't in the set. Once we take this difference, we either end up with 0 (x is in the set) or nonzero (this is the extra orthogonal part, so we have a new orthogonal vector to add to the set.)
\section {Gram-Schmidt Algorithm}
Let $x_1 ... x_k$ be a set of vectors.
$f_1 = x_1$
$f_2 = x_2 - proj_{f_1} x_2$
$f_3 = x_3 - proj_{f_1} x_3 - proj_{f_2} x_3$ ... we keep on taking projections and stripping away orthogonal bits until:\\ we get a zero. Generally:
$f_k = x_k = \sum_i^k-1 proj_{f_i}x_k$\\\\

BREAK \\\\

Proof of Part 2: Give a subspace U and vector v, let p be the sum of the projections of v onto some orthogonal basis f1 ... fn of U. (2) Then (v-p) is perpendicular to p, and |v-p| is minimized. The proof of part (2):\\
$x = p + (x-p) \\ |v-x| = |v-(p-(x-p))| \\ = |v-p - (x-p) |$ \\ v-p (in U perp) and x-p in U. We square this and get  $|v-p|^2 + |x-p|^2$ by pythagoreas;\\ will be greater than $|v-p|^2$.\\\\THM: Let U be a subspace in Rn, and T(x) be $proj_U(\vec x)$.\\ \begin{enumerate}
	\item T(x) is linear
	\item im(T) = U (all the projections of something into a subspace form the subspace)
	\item ker(T) $\sim$ null(T), but kernel is the nomenclature for a transformation. This is equal to $U^\perp$. Thus, we know that $$dim(U) + dim(U^\perp) = n$$ for $\mathbb{R}^n$
\end{enumerate}

ex. Find $U^\perp$ for U = span (1 3 -1)T, (2 1 1)T, so that $U^\perp = [x y z] \cdot$ the vectors in the span are zero. So we're finding the null space of $\begin{bmatrix} 1& 3 &-1 \\2 & 1 & 1\end{bmatrix}$. Alternatively, in R3 you could just take the cross product of two vectors. \\\\

Let A = [x1, x2 ... xn] nxn matrix, xi are vectors then
$A^TA = \begin{bmatrix} \vec x_i^T \cdot \vec x_j\end{bmatrix}$\\ If $A^TA = I$, so the $A^{-1} = A^T$ then $\vec x_i^T \cdot \vec x_j$ = 0 if i isn't j, and 1 if i is itself. (Dirac Delta Function $\delta_{ij}$). We get orthonormal vectors like this. \\\\ As a theorem, for any nxn matrix P (P, because they tend to be related ot projections), the following are equivalent
\begin{enumerate}
	\item P is invertible with $inv(P) = P^T$
	\item The columns of P are orthonormal
	\item The rows of P are orthonormal
\end{enumerate}
Any such matrix is called orthogonal (though, perhaps we should call them orthonormal).\\\\

\subsection{Orthogonal Diagonalization}
Let A be diagonalizable (A = $PDP^{-1}$) where there is a choice of the matrix P, such that it is orthogonal. \\ This tells us $A = PDP^T$. IF we take the transpose of that, we get $A^T = (P^T)T D^T P^T = A$. So A has to be symmetric. ($D^T$ is D) \\\\
\subsection{Spectral Axis Theorem}
Every symmetric matrix can be diagonalized by a orthogonal matrix. \\\\ Def: A is orthogonally diagonalizable if there exists an orthogonal matrix P such that D = $P^{-1}AP = P^TAP$. For this to happen, P is diagonalizing implies that the columns of P are eigenvectors, and P is orthogonal suggests the cols of P are orthonormal. Therefore, we need an orthonormal set of n eigenvectors.\\\\ Thm: If A can be Orth. Diagonalized, and $\alpha$ and $\beta$ are distinct eigenvalues with eigenvectors $x,y$ respectively, then x is orthogonal to y (They come pre-orthogonalized, you don't need to use GSA). \\ PROOF \\ What happens when we take a transform on one side of a dot product and move it to the other side:
$Ax = \alpha x$ $Ay = \beta y$ We'll start with $Ax \cdot y \\ = \alpha x \cdot y \\ (Ax)^T y = x^T A^T y = x^T(A^T y = x(Ay) = x(\beta y)$. A is symmetric so the transpose is equal to itself. Since alpha and beta give the same output, we know they are zero, and the dot product is zero so they're orthogonal.

\subsubsection{Process}
\begin{enumerate}
	\item Find the eigenvalues and eigenvectors as done before
	\item For every eigenvalue with multiplicity > 1, use the GSA to create an orthogonal basis
	\item From this, we get an orthogonal, but not orthonormal basis for the matrix, so now we just need to divide every basic eigenvector by its magnitude to fufill orthonormality.
\end{enumerate}
The problem is that, if the matrix is not symmetric, we cannot use this. If the matrix is not symmetric, you can \textit{ONLY USE REGULAR DIAGONALIZATION!}

\end{document}
